# ğŸ Snake AI - Apprentissage par Renforcement

Un projet d'intelligence artificielle qui apprend Ã  jouer au jeu Snake en utilisant le **Reinforcement Learning** avec l'algorithme **PPO** (Proximal Policy Optimization).

![Python](https://img.shields.io/badge/Python-3.14-blue)
![Stable Baselines3](https://img.shields.io/badge/Stable--Baselines3-2.7.1-green)
![Gymnasium](https://img.shields.io/badge/Gymnasium-1.2.3-orange)
![Pygame](https://img.shields.io/badge/Pygame-2.6.1-red)

---

## ğŸ“‹ Table des matiÃ¨res

- [Description](#-description)
- [Structure du projet](#-structure-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Environnements](#-environnements)
- [Architecture du modÃ¨le](#-architecture-du-modÃ¨le)
- [RÃ©sultats](#-rÃ©sultats)

---

## ğŸ“– Description

Ce projet implÃ©mente un agent d'apprentissage par renforcement capable de jouer au jeu Snake de maniÃ¨re autonome. L'agent apprend Ã  :
- ğŸ Trouver et manger les pommes
- ğŸ§± Ã‰viter les murs
- ğŸ Ne pas se mordre la queue

L'apprentissage utilise l'algorithme **PPO** de la librairie Stable-Baselines3, reconnu pour sa stabilitÃ© et ses bonnes performances.

---

## ğŸ“ Structure du projet

```
projet_snake/
â”œâ”€â”€ envs/                       # Environnements Gymnasium
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ snake_env.py           # Env V1 : Observation = vecteur 11 valeurs
â”‚   â””â”€â”€ snake_env_cnn.py       # Env V2 : Observation = grille 30x30 (CNN)
â”œâ”€â”€ checkpoints/               # ModÃ¨les sauvegardÃ©s (.zip)
â”‚   â””â”€â”€ PPO/
â”œâ”€â”€ logs/                      # Logs TensorBoard
â”œâ”€â”€ train_v1.py               # EntraÃ®nement basique (100k steps)
â”œâ”€â”€ train_v2.py               # EntraÃ®nement avancÃ© (2M steps)
â”œâ”€â”€ train_v3.py               # EntraÃ®nement CNN
â”œâ”€â”€ test_play.py              # Visualiser l'IA jouer
â”œâ”€â”€ check_env.py              # VÃ©rifier l'environnement
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### 1. Cloner le projet
```bash
git clone https://github.com/SAMY-EH/SY23P.git
cd SY23P/projet_ultra_secret/projet_snake
```

### 2. CrÃ©er un environnement virtuel
```bash
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# ou
.venv\Scripts\activate     # Windows
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

> âš ï¸ **macOS** : Si pygame ne s'installe pas, installez d'abord SDL2 :
> ```bash
> brew install sdl2 sdl2_image sdl2_mixer sdl2_ttf sdl2_gfx
> ```

---

## ğŸ® Utilisation

### EntraÃ®ner l'IA

```bash
# EntraÃ®nement basique (100 000 steps) - ~5 min
python train_v1.py

# EntraÃ®nement avancÃ© (500 000 steps) - ~20 min
python train_v2.py

# EntraÃ®nement avec CNN (plus lent mais potentiellement meilleur)
python train_v3.py
```

### Voir l'IA jouer

```bash
python test_play.py
```

Un menu s'affiche pour choisir le modÃ¨le Ã  charger parmi ceux disponibles dans `checkpoints/`.

### Visualiser les logs d'entraÃ®nement

```bash
tensorboard --logdir=logs
```
Puis ouvrir http://localhost:6006 dans un navigateur.

---

## ğŸŒ Environnements

### `SnakeEnv` (snake_env.py)
- **Observation** : Vecteur de 11 valeurs binaires
  - 3 valeurs : Danger (tout droit, droite, gauche)
  - 4 valeurs : Direction actuelle (G, D, H, B)
  - 4 valeurs : Position relative de la pomme (G, D, H, B)
- **Actions** : 4 (Gauche, Droite, Haut, Bas)
- **RÃ©compenses** :
  - +10 : Manger une pomme
  - -10 : Collision (mur ou queue)

### `SnakeEnvCnn` (snake_env_cnn.py)
- **Observation** : Image 30x30 en niveaux de gris
  - 0 : Case vide
  - 80 : Corps du serpent
  - 180 : TÃªte
  - 255 : Pomme
- **RÃ©seau** : CNN (CnnPolicy) pour traiter l'image

---

## ğŸ§  Architecture du modÃ¨le

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PPO Agent                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Policy Network (MlpPolicy)                         â”‚
â”‚  â”œâ”€â”€ Input Layer (11 neurons)                       â”‚
â”‚  â”œâ”€â”€ Hidden Layer 1 (64 neurons, ReLU)             â”‚
â”‚  â”œâ”€â”€ Hidden Layer 2 (64 neurons, ReLU)             â”‚
â”‚  â””â”€â”€ Output Layer (4 neurons, Softmax)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Value Network                                      â”‚
â”‚  â”œâ”€â”€ Shared layers with Policy                      â”‚
â”‚  â””â”€â”€ Output (1 neuron - state value)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š RÃ©sultats

| Version | Steps | Score Moyen | Temps |
|---------|-------|-------------|-------|
| V1 (MLP) | 100k | ~5-10 | 5 min |
| V2 (MLP) | 500k | ~15-25 | 20 min |
| V3 (CNN) | 1M | ~20-30 | 1h+ |

> Les rÃ©sultats peuvent varier selon les hyperparamÃ¨tres et la configuration matÃ©rielle.

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **[Gymnasium](https://gymnasium.farama.org/)** : Framework pour environnements RL
- **[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)** : Algorithmes RL (PPO, DQN, A2C...)
- **[PyTorch](https://pytorch.org/)** : Backend deep learning
- **[Pygame](https://www.pygame.org/)** : Rendu graphique du jeu
- **[TensorBoard](https://www.tensorflow.org/tensorboard)** : Visualisation des mÃ©triques

---

## ğŸ“ Auteur

**Samy E et Willen A** - Projet SY23 - Janvier 2026

---

## ğŸ“œ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'un cours universitaire.

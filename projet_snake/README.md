# ğŸ Snake AI - Apprentissage par Renforcement

Un projet d'intelligence artificielle qui apprend Ã  jouer au jeu Snake en utilisant le **Reinforcement Learning** avec l'algorithme **PPO** (Proximal Policy Optimization).

![Python](https://img.shields.io/badge/Python-3.12-blue)
![Stable Baselines3](https://img.shields.io/badge/Stable--Baselines3-2.0+-green)
![Gymnasium](https://img.shields.io/badge/Gymnasium-1.0+-orange)
![Pygame](https://img.shields.io/badge/Pygame-2.6+-red)

---

## ğŸ“‹ Table des matiÃ¨res

- [Description](#-description)
- [Structure du projet](#-structure-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Environnements](#-environnements)
- [Architecture du modÃ¨le](#-architecture-du-modÃ¨le)
- [RÃ©sultats](#-rÃ©sultats)

---

## ğŸ“– Description

Ce projet implÃ©mente un agent d'apprentissage par renforcement capable de jouer au jeu Snake de maniÃ¨re autonome. L'agent apprend Ã  :
- ğŸ Trouver et manger les pommes
- ğŸ§± Ã‰viter les murs
- ğŸ Ne pas se mordre la queue

L'apprentissage utilise l'algorithme **PPO** de la librairie Stable-Baselines3, reconnu pour sa stabilitÃ© et ses bonnes performances.

---

## ğŸ“ Structure du projet

```
projet_snake/
â”œâ”€â”€ envs/                       # Environnements Gymnasium
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ snake_env.py           # Env V1 : Observation = vecteur 11 valeurs (MLP)
â”‚   â””â”€â”€ snake_env_cnn.py       # Env V2 : Observation = grille 30x30 (CNN)
â”œâ”€â”€ checkpoints/               # ModÃ¨les sauvegardÃ©s (.zip)
â”‚   â”œâ”€â”€ PPO/                   # ModÃ¨les MLP
â”‚   â””â”€â”€ PPO_CNN/               # ModÃ¨les CNN
â”œâ”€â”€ logs/                      # Logs TensorBoard
â”œâ”€â”€ train_v1.py               # EntraÃ®nement basique MLP (100k steps)
â”œâ”€â”€ train_v2.py               # EntraÃ®nement avancÃ© MLP (500k steps)
â”œâ”€â”€ train_v3.py               # EntraÃ®nement CNN avec parallÃ©lisation
â”œâ”€â”€ train_colab.ipynb         # Notebook pour Google Colab (GPU)
â”œâ”€â”€ test_play.py              # Visualiser l'IA MLP jouer
â”œâ”€â”€ test_play_cnn.py          # Visualiser l'IA CNN jouer
â”œâ”€â”€ check_env.py              # VÃ©rifier l'environnement
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### 1. Cloner le projet
```bash
git clone https://github.com/SAMY-EH/SY23_V2.git
cd SY23_V2/projet_snake
```

### 2. CrÃ©er un environnement virtuel
```bash
python -m venv .venv
source .venv/bin/activate  # macOS/Linux
# ou
.venv\Scripts\activate     # Windows
```

### 3. Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

> âš ï¸ **macOS** : Si pygame ne s'installe pas, installez d'abord SDL2 :
> ```bash
> brew install sdl2 sdl2_image sdl2_mixer sdl2_ttf sdl2_gfx
> ```

---

## ğŸ® Utilisation

### EntraÃ®ner l'IA

```bash
# EntraÃ®nement MLP basique (100 000 steps) - ~5 min
python train_v1.py

# EntraÃ®nement MLP avancÃ© (500 000 steps) - ~20 min
python train_v2.py

# EntraÃ®nement CNN avec parallÃ©lisation (plus lent mais plus gÃ©nÃ©ral)
python train_v3.py
```

### Voir l'IA jouer

```bash
# Version MLP (vecteur 11 valeurs)
python test_play.py

# Version CNN (grille 30x30)
python test_play_cnn.py
```

Un menu s'affiche pour choisir le modÃ¨le Ã  charger parmi ceux disponibles dans `checkpoints/`.

### Visualiser les logs d'entraÃ®nement

```bash
tensorboard --logdir=logs
```
Puis ouvrir http://localhost:6006 dans un navigateur.

### EntraÃ®nement sur Google Colab (GPU)

1. Ouvrir `train_colab.ipynb` sur Google Colab
2. Activer le GPU : `ExÃ©cution > Modifier le type d'exÃ©cution > T4 GPU`
3. ExÃ©cuter les cellules dans l'ordre

---

## ğŸŒ Environnements

### `SnakeEnv` (snake_env.py) - Version MLP

| CaractÃ©ristique | Description |
|-----------------|-------------|
| **Observation** | Vecteur de 11 valeurs binaires |
| **Espace** | `Box(0, 1, shape=(11,), dtype=int8)` |
| **Actions** | 4 : Gauche, Droite, Haut, Bas |

**DÃ©tail du vecteur d'observation :**
- 3 valeurs : Danger (tout droit, droite, gauche)
- 4 valeurs : Direction actuelle (G, D, H, B)
- 4 valeurs : Position relative de la pomme (G, D, H, B)

**RÃ©compenses :**
- `+10` : Manger une pomme
- `-10` : Collision (mur ou queue)

---

### `SnakeEnvCnn` (snake_env_cnn.py) - Version CNN

| CaractÃ©ristique | Description |
|-----------------|-------------|
| **Observation** | Grille 30x30 en niveaux de gris |
| **Espace** | `Box(0, 255, shape=(1, 30, 30), dtype=uint8)` |
| **Actions** | 4 : Gauche, Droite, Haut, Bas |

**Valeurs de la grille :**
- `0` : Case vide (noir)
- `80` : Corps du serpent (gris foncÃ©)
- `180` : TÃªte du serpent (gris clair)
- `255` : Pomme (blanc)

**RÃ©compenses avec reward shaping :**
- `+20` : Manger une pomme
- `-10` : Collision (mur ou queue)
- `+1` : Se rapprocher de la pomme
- `-1` : S'Ã©loigner de la pomme

---

## ğŸ§  Architecture du modÃ¨le

### Version MLP (train_v1.py, train_v2.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PPO Agent                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Policy Network (MlpPolicy)                         â”‚
â”‚  â”œâ”€â”€ Input Layer (11 neurons)                       â”‚
â”‚  â”œâ”€â”€ Hidden Layer 1 (64 neurons, ReLU)             â”‚
â”‚  â”œâ”€â”€ Hidden Layer 2 (64 neurons, ReLU)             â”‚
â”‚  â””â”€â”€ Output Layer (4 neurons, Softmax)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Value Network                                      â”‚
â”‚  â”œâ”€â”€ Shared layers with Policy                      â”‚
â”‚  â””â”€â”€ Output (1 neuron - state value)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Version CNN (train_v3.py)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Custom CNN Extractor                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conv2D(1 â†’ 32, kernel=3, stride=2) + ReLU         â”‚
â”‚  Conv2D(32 â†’ 64, kernel=3, stride=2) + ReLU        â”‚
â”‚  Conv2D(64 â†’ 64, kernel=3, stride=1) + ReLU        â”‚
â”‚  Flatten â†’ Linear â†’ 128 features                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Policy Head: 128 â†’ 4 (actions)                    â”‚
â”‚  Value Head: 128 â†’ 1 (state value)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š RÃ©sultats

| Version | Environnement | Steps | Score Moyen | Temps |
|---------|---------------|-------|-------------|-------|
| V1 (MLP) | SnakeEnv | 100k | ~5-10 | ~5 min |
| V2 (MLP) | SnakeEnv | 500k | ~15-25 | ~20 min |
| V3 (CNN) | SnakeEnvCnn | 1M+ | ~20-30 | 1h+ (GPU recommandÃ©) |

> Les rÃ©sultats peuvent varier selon les hyperparamÃ¨tres et la configuration matÃ©rielle.

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **[Gymnasium](https://gymnasium.farama.org/)** : Framework pour environnements RL
- **[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)** : Algorithmes RL (PPO)
- **[PyTorch](https://pytorch.org/)** : Backend deep learning
- **[Pygame](https://www.pygame.org/)** : Rendu graphique du jeu
- **[TensorBoard](https://www.tensorflow.org/tensorboard)** : Visualisation des mÃ©triques

---

## ğŸ“ Auteurs

**Samy E. et Willen A.** - Projet SY23 - UTC - Janvier 2026

---

## ğŸ“œ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'un cours universitaire (SY23 - UTC).